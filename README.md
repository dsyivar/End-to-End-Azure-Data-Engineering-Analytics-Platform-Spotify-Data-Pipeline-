# AzureProject

I designed and implemented an end-to-end Azure data platform using Azure Data Factory, Azure Databricks, Azure SQL Database, and Delta Lake, following industry best practices for scalable and reliable data engineering. Furthermore, I built incremental and metadata-driven ingestion pipelines in Azure Data Factory, incorporating backfilling logic and looping constructs to efficiently process new and changed data from source systems. Also, i developed Databricks notebooks using PySpark to perform complex transformations, applying star schema modeling and Slowly Changing Dimensions (SCD) to produce analytics-ready datasets. Next, I enabled real-time streaming processing through Spark Structured Streaming and Databricks Autoloader, and implemented Delta Live Tables with data quality expectations to ensure reliable and validated lakehouse pipelines. I also managed deployments, configurations, and reusable Python utilities using Databricks Asset Bundles and GitHub-based version control to support maintainability and reproducibility across environments.
